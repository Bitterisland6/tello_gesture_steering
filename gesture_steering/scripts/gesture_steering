#!/usr/bin/env python3

import sys
import argparse

import rclpy
from rclpy.utilities import remove_ros_args
from rclpy.node import Node
from rclpy.parameter import Parameter
import sensor_msgs.msg
import std_msgs.msg
import geometry_msgs.msg
import cv2
import cv_bridge

import tflite_runtime.interpreter as tflite
import numpy as np

import hand_tracking as ht
import math_gesture_interpreter as mgi


class GestureSteering(Node):
    def __init__(self, use_model, modelPath):
        super().__init__("static_gesture_steering")
        self.get_params()

        self.bridge = cv_bridge.CvBridge()

        # initialization of interpretation method based on use_model argument
        self.use_model = use_model
        if use_model:
            # using neural network model
            self.get_logger().info("Using model for gesture recognition.")
            try:
                self.interpreting_func = self.model_interpretation
                self.interpreter = tflite.Interpreter(model_path=modelPath)
                self.input_details = self.interpreter.get_input_details()
                self.output_details = self.interpreter.get_output_details()
                self.interpreter.allocate_tensors()
            except ValueError as e:
                raise Exception(f"Couldnt load tflite model: {modelPath}")
        else:
            # using math interpreter
            self.interpreting_func = self.math_interpretation
            self.interpreter = mgi.Interpreter()
            self.get_logger().info("Using math interpreter for gesture recognition.")

        # flag for stopping the drone if the interpreted gesture doesn't move the drone
        self.vel_gesture = False

        # ros publishers
        self.land_pub = self.create_publisher(std_msgs.msg.Empty, "land", 10)
        self.cmd_vel_pub = self.create_publisher(geometry_msgs.msg.Twist, "cmd_vel", 10)
        self.flip_pub = self.create_publisher(std_msgs.msg.String, "flip", 10)

        # ros subscribers
        self.img_sub = self.create_subscription(
            sensor_msgs.msg.Image, "drone_camera/image", self.img_callback, 1
        )

        self.land_count = 0

        self.get_logger().info("Starting static_gesture_steering node.")

    def get_params(self):
        """Function loading ros parameters from yaml file"""

        self.declare_parameters(
            namespace="",
            parameters=[
                ("linear_y_const", 0.2),
                ("linear_x_const", 0.2),
                ("linear_z_const", 0.5),
                ("angular_z_const", 1.0),
                ("confidence_threshold", 0.95),
                ("gestures", ["none"]),
                ("width", 640),
                ("height", 480),
                ("min_detection_confidence", 0.5),
                ("min_tracking_confidence", 0.5),
                ("draw", False),
            ],
        )

        # velocities parameters
        self.linear_y_const = float(self.get_parameter("linear_y_const").value)
        self.linear_x_const = float(self.get_parameter("linear_x_const").value)
        self.linear_z_const = float(self.get_parameter("linear_z_const").value)
        self.angular_z_const = float(self.get_parameter("angular_z_const").value)

        # confidence threshold for detection
        self.threshold = float(self.get_parameter("confidence_threshold").value)

        # gestures instructions
        gestures = self.get_parameter("gestures").value

        if len(gestures) == 1:
            raise Exception("Couldn't load parameters.")

        self.steering_arguments = dict()
        self.gesture_to_id = dict()
        self.id_to_gesture = dict()

        for gesture in gestures:
            param_type = (
                Parameter.Type.INTEGER_ARRAY
                if "flip" not in gesture
                else Parameter.Type.STRING_ARRAY
            )
            self.declare_parameter("func_arguments." + gesture, param_type)

        # mapping gesture to id and saving instructions for each gesture
        for id_, gesture in enumerate(gestures):
            self.steering_arguments[id_] = self.get_parameter(
                "func_arguments." + gesture
            ).value
            self.gesture_to_id[gesture] = id_
            self.id_to_gesture[id_] = gesture

        # hand tracker parameters
        width = int(self.get_parameter("width").value)
        height = int(self.get_parameter("height").value)
        min_det_conf = float(self.get_parameter("min_detection_confidence").value)
        min_trac_conf = float(self.get_parameter("min_tracking_confidence").value)
        draw = bool(self.get_parameter("draw").value)

        self.tracker = ht.HandTracker(
            width,
            height,
            draw=draw,
            min_detection_confidence=min_det_conf,
            min_tracking_confidence=min_trac_conf,
        )

    def model_interpretation(self, normalized_landmarks):
        """Function getting model interpretation of the gesture
        from landmarks acquired from hand tracker.

        Args:
            normalized_landmarks: hand landmarks in raw format from hand tracker
        Returns:
            class_id: id of a detected gesture
            val: model confidence of its decision
        """
        # parsing landmarks to operational format
        landmarks = parse_landmarks(normalized_landmarks)

        # input landmarks to model
        self.interpreter.set_tensor(
            self.input_details[0]["index"], [landmarks.astype(np.float32)]
        )
        # running inference
        self.interpreter.invoke()
        # getting prediction
        prediction = self.interpreter.get_tensor(self.output_details[0]["index"])
        class_id = np.argmax(prediction[0])
        val = prediction[0][class_id]

        return class_id, val

    def math_interpretation(self, landmarks):
        """Function gettin math interpretation of the gesture
        from scaled landmarks (positions are in image coordinates,
        not in the range of [0,1]) acquired from hand tracker.

        Args:
            landmarks: hand landmarks coordinates from hand tracker,
            that are scaled to image size and in operational format (list of tuples)
        Returns:
            A tuple of gesture indentificator and confidence, where:
                * identificator is either id (successful recognition)
                  or "none" (failed to recognize gesture)
                * confidence is either 1.0 or 0.0 (success or failure in recognition)
        """
        status = self.interpreter.get_fingers_open(landmarks)

        if sum(status) == 2 and status[2] and status[4]:
            return self.gesture_to_id["land"], 1.0

        if self.interpreter.gesture_up(landmarks):
            self.vel_gesture = True
            return self.gesture_to_id["up"], 1.0

        elif self.interpreter.gesture_down(landmarks):
            self.vel_gesture = True
            return self.gesture_to_id["down"], 1.0

        elif self.interpreter.gesture_left(landmarks):
            self.vel_gesture = True
            return self.gesture_to_id["left"], 1.0

        elif self.interpreter.gesture_right(landmarks):
            self.vel_gesture = True
            return self.gesture_to_id["right"], 1.0

        elif self.vel_gesture:
            self.vel_gesture = False
            self.pub_velocity()

        if self.interpreter.flip_front(landmarks):
            return self.gesture_to_id["flip_front"], 1.0

        elif self.interpreter.flip_back(landmarks):
            return self.gesture_to_id["flip_back"], 1.0

        elif self.interpreter.flip_left(landmarks):
            return self.gesture_to_id["flip_right"], 1.0

        elif self.interpreter.flip_right(landmarks):
            return self.gesture_to_id["flip_left"], 1.0

        return "none", 0.0

    def get_steering(self, img):
        """Function that runs hand detection on the image
        and gesture interpretation using implemented methods.
        Also gives instructions to the robot based on the interpretation.

        Args:
            img: opencv format image from robot camera
        Returns:
            gesture: id of the interpreted gesture
            confidence: confidence of the interpretation
        """

        landmarks = self.tracker.get_landmarks(img, normalized=self.use_model)

        # checking if the hand got detected
        if not landmarks:
            self.pub_velocity()
            return 9, 0.0

        # getting the interpretation
        gesture, confidence = self.interpreting_func(landmarks)

        # checking if the confidence is high enough
        if confidence < self.threshold:
            self.get_logger().info("No gesture detected")
            self.pub_velocity()
            return 9, 0.0

        self.get_logger().info(
            f"Interpreted gesture: {self.id_to_gesture[gesture]}, confidence: {round(confidence*100, 2)}%"
        )

        # increasing number of "land" detection in a row - prevents random detections
        if self.id_to_gesture[gesture] == "land":
            self.land_count += 1
        else:
            self.land_count = 0

        # self.gesture_to_func[gesture](*self.steering_arguments[gesture])
        # operating the drone based on the interpretation
        self.command(self.steering_arguments[gesture])

        return gesture, confidence

    def img_callback(self, data: sensor_msgs.msg.Image):
        """Function called each time a message on ros topic with camera data comes.
        Puts information text on the image with detected gesture and confidence.
        """
        cv_img = self.bridge.imgmsg_to_cv2(data, "passthrough")

        gesture, confidence = self.get_steering(cv_img)
        msg = f"Gesture: {self.id_to_gesture[gesture]}, confidence: {round(confidence*100, 2)}%"
        cv2.putText(cv_img, msg, (10, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 0, 0), 2)
        cv2.putText(
            cv_img, msg, (10, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 0, 255), 1
        )

        cv2.imshow("img", cv_img)
        cv2.waitKey(1)

    def command(self, args):
        """Function invoking right method based on the arguments that it got.
        Works as an universal function for commanding the robot.
        Methods for specific actions are invoked if the arguments are in right format:
            * one-element list, where the element is one of "l", "f", "r", "b" - flip function
                The letters represent flip direction (left, front, right, back)
            * list with 4 floats - pub_velocity function
                The numbers represent values for speed in different axes for linear and angular speeds in such order:
                (linear x, linear y, linear z, angular z)
            * one-element list, where the element is of any type
              and different from "l", "f", "r" and "b" - land function (if the gesture was detected couple times in a row)
        Arg:
            args: list of arguments that will be provided to function with specific instructions.
        """
        # checking if flip instruction
        if args[0] in ["l", "f", "r", "b"]:
            self.flip(*args)
        # checking if movement instruction
        elif len(args) > 1:
            self.pub_velocity(*args)
        # checking if land instruction
        elif self.land_count > 5:
            self.land_count = 0
            self.land(*args)

    def pub_velocity(self, linear_x=0.0, linear_y=0.0, linear_z=0.0, angular_z=0.0):
        """Function performing movement operation of the robot.
        Publishes Twist message to "cmd_vel" topic.

        Args:
            linear_x: value of linear speed in x axis (gets multiplied by constant from parameter)
            linear_y: value of linear speed in y axis (gets multiplied by constant from parameter)
            linear_z: value of linear speed in z axis (gets multiplied by constant from parameter)
            angular_z: value of angular speed in z axis (gets multiplied by constant from parameter)
        """
        vel_msg = geometry_msgs.msg.Twist()

        vel_msg.angular.z = angular_z * self.angular_z_const
        vel_msg.linear.x = linear_x * self.linear_x_const
        vel_msg.linear.y = linear_y * self.linear_y_const
        vel_msg.linear.z = linear_z * self.linear_z_const

        self.cmd_vel_pub.publish(vel_msg)

    def flip(self, direction):
        """Function performing flip operation of the robot.
        Publishes String message to "flip" topic.

        Args:
            direction: a single letter ("f"/"b"/"l"/"r") representing flip direction (front, back, left, right)
        """
        msg = std_msgs.msg.String()
        msg.data = direction
        self.flip_pub.publish(msg)

    def land(self, empty):
        """Function performig land operation of the robot.
        Publishes empty message on the "land" topic (works like a trigger).
        Args:
            empty: just a placeholder needed for the code structure,
            not used in the function as the "land" topic is of type std_msgs.Empty"""
        msg = std_msgs.msg.Empty()
        self.land_pub.publish(msg)


def add_arguments(parser: argparse.ArgumentParser):
    parser.add_argument(
        "-m",
        "--use-model",
        default="False",
        type=str,
        nargs="?",
        dest="use_model",
        help="use neural network model for gesture recognition",
    )

    parser.add_argument(
        "-p",
        "--path",
        default="",
        type=str,
        nargs="?",
        dest="model_path",
        help="path to the neural network model",
    )


def parse_landmarks(landmarks):
    """Function parsing landmarks to operational format.
    Landmarks returned from hand tracker are a dictionary.
    Returned value of this function is a numpy array,
    with landmark coordinates (x, y, z).

    Args:
        landmarks: landmarks returned from hand tracker in original format
    Returns:
        ret: numpy array of shape (21,3) with landmarks coordinates"""

    ret = []
    for landmark in landmarks.landmark:
        ret.append([landmark.x, landmark.y, landmark.z])

    return np.array(ret)


if __name__ == "__main__":
    rclpy.init()

    parser = argparse.ArgumentParser(
        description="Operate the drone using hand gestures."
    )
    add_arguments(parser)

    rosargv = remove_ros_args(sys.argv)[1:]
    args = parser.parse_args(rosargv)

    steering = GestureSteering(args.use_model.lower() == "true", args.model_path)

    try:
        rclpy.spin(steering)
    except KeyboardInterrupt as e:
        steering.get_logger().info("Got Ctrl+C, shutting down.")
        steering.land([])
        steering.destroy_node()
    finally:
        rclpy.shutdown()
