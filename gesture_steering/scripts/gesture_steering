#!/usr/bin/env python3

import sys
import argparse
from collections import deque

import rclpy
from rclpy.utilities import remove_ros_args
from rclpy.node import Node
from rclpy.parameter import Parameter
import sensor_msgs.msg
import std_msgs.msg
import geometry_msgs.msg
import cv2
import cv_bridge

import tflite_runtime.interpreter as tflite
import numpy as np

import hand_tracking as ht
import math_gesture_interpreter as mgi


class GestureSteering(Node):
    def __init__(self, use_model, modelPath):
        super().__init__("static_gesture_steering")
        self.get_params()

        # self.gesture_to_func = {
        #     # 0: lambda: self.pub_velocity(0, 0, 0, 0),
        #     0: self.pub_velocity,
        #     1: self.pub_velocity,
        #     2: self.pub_velocity,
        #     3: self.pub_velocity,
        #     4: self.flip,
        #     5: self.flip,
        #     6: self.flip,
        #     7: self.flip,
        #     8: self.land,
        #     9: self.pub_velocity,
        # }

        self.bridge = cv_bridge.CvBridge()

        self.use_model = use_model
        if use_model:
            self.get_logger().info("Using model for gesture recognition.")
            try:
                self.interpreting_func = self.model_interpretation
                self.interpreter = tflite.Interpreter(model_path=modelPath)
                self.input_details = self.interpreter.get_input_details()
                self.output_details = self.interpreter.get_output_details()
                self.interpreter.allocate_tensors()
            except ValueError as e:
                raise Exception(f"Couldnt load tflite model: {modelPath}")
        else:
            self.interpreting_func = self.math_interpretation
            self.interpreter = mgi.Interpreter()
            self.get_logger().info("Using math interpreter for gesture recognition.")

        self.vel_gesture = False

        # ros publishers
        self.land_pub = self.create_publisher(std_msgs.msg.Empty, "land", 10)
        self.cmd_vel_pub = self.create_publisher(geometry_msgs.msg.Twist, "cmd_vel", 10)
        self.flip_pub = self.create_publisher(std_msgs.msg.String, "flip", 10)

        # ros subscribers
        self.img_sub = self.create_subscription(
            sensor_msgs.msg.Image, "drone_camera/image", self.img_callback, 1
        )

        self.land_count = 0

        self.get_logger().info("Starting static_gesture_steering node.")

    def get_params(self):
        self.declare_parameters(
            namespace="",
            parameters=[
                ("linear_y_const", 0.2),
                ("linear_x_const", 0.2),
                ("linear_z_const", 0.5),
                ("angular_z_const", 1.0),
                ("confidence_threshold", 0.95),
                ("gestures", ["none"]),
                ("width", 640),
                ("height", 480),
                ("min_detection_confidence", 0.5),
                ("min_tracking_confidence", 0.5),
                ("draw", False),
            ],
        )

        self.linear_y_const = float(self.get_parameter("linear_y_const").value)
        self.linear_x_const = float(self.get_parameter("linear_x_const").value)
        self.linear_z_const = float(self.get_parameter("linear_z_const").value)
        self.angular_z_const = float(self.get_parameter("angular_z_const").value)

        self.threshold = float(self.get_parameter("confidence_threshold").value)

        gestures = self.get_parameter("gestures").value

        if len(gestures) == 1:
            raise Exception("Couldn't load parameters.")

        self.steering_arguments = dict()
        self.gesture_to_id = dict()
        self.id_to_gesture = dict()

        for gesture in gestures:
            param_type = (
                Parameter.Type.INTEGER_ARRAY
                if "flip" not in gesture
                else Parameter.Type.STRING_ARRAY
            )
            self.declare_parameter("func_arguments." + gesture, param_type)

        for id_, gesture in enumerate(gestures):
            self.steering_arguments[id_] = self.get_parameter(
                "func_arguments." + gesture
            ).value
            self.gesture_to_id[gesture] = id_
            self.id_to_gesture[id_] = gesture

        width = int(self.get_parameter("width").value)
        height = int(self.get_parameter("height").value)
        min_det_conf = float(self.get_parameter("min_detection_confidence").value)
        min_trac_conf = float(self.get_parameter("min_tracking_confidence").value)
        draw = bool(self.get_parameter("draw").value)

        self.tracker = ht.HandTracker(
            width,
            height,
            draw=draw,
            min_detection_confidence=min_det_conf,
            min_tracking_confidence=min_trac_conf,
        )

    def model_interpretation(self, normalized_landmarks):
        landmarks = parse_landmarks(normalized_landmarks)
        self.interpreter.set_tensor(
            self.input_details[0]["index"], [landmarks.astype(np.float32)]
        )
        self.interpreter.invoke()
        prediction = self.interpreter.get_tensor(self.output_details[0]["index"])
        class_id = np.argmax(prediction[0])
        val = prediction[0][class_id]

        return class_id, val

    def math_interpretation(self, landmarks):
        status = self.interpreter.get_fingers_open(landmarks)

        if sum(status) == 2 and status[2] and status[4]:
            return self.gesture_to_id["land"], 1.0

        if self.interpreter.gesture_up(landmarks):
            self.vel_gesture = True
            return self.gesture_to_id["up"], 1.0

        elif self.interpreter.gesture_down(landmarks):
            self.vel_gesture = True
            return self.gesture_to_id["down"], 1.0

        elif self.interpreter.gesture_left(landmarks):
            self.vel_gesture = True
            return self.gesture_to_id["left"], 1.0

        elif self.interpreter.gesture_right(landmarks):
            self.vel_gesture = True
            return self.gesture_to_id["right"], 1.0

        elif self.vel_gesture:
            self.vel_gesture = False
            self.pub_velocity()

        if self.interpreter.flip_front(landmarks):
            return self.gesture_to_id["flip_front"], 1.0

        elif self.interpreter.flip_back(landmarks):
            return self.gesture_to_id["flip_back"], 1.0

        elif self.interpreter.flip_left(landmarks):
            return self.gesture_to_id["flip_right"], 1.0

        elif self.interpreter.flip_right(landmarks):
            return self.gesture_to_id["flip_left"], 1.0

        return "none", 0.0

    def get_steering(self, img):
        landmarks = self.tracker.get_landmarks(img, normalized=self.use_model)

        if not landmarks:
            self.pub_velocity()
            return 9, 0.0

        gesture, confidence = self.interpreting_func(landmarks)

        if confidence < self.threshold:
            self.get_logger().info("No gesture detected")
            self.pub_velocity()
            return 9, 0.0

        self.get_logger().info(
            f"Interpreted gesture: {self.id_to_gesture[gesture]}, confidence: {round(confidence*100, 2)}%"
        )

        if self.id_to_gesture[gesture] == "land":
            self.land_count += 1
        else:
            self.land_count = 0

        #self.gesture_to_func[gesture](*self.steering_arguments[gesture])
        self.command(self.steering_arguments[gesture])

        return gesture, confidence

    def img_callback(self, data: sensor_msgs.msg.Image):
        cv_img = self.bridge.imgmsg_to_cv2(data, "passthrough")

        gesture, confidence = self.get_steering(cv_img)
        msg = f"Gesture: {self.id_to_gesture[gesture]}, confidence: {round(confidence*100, 2)}%"
        cv2.putText(cv_img, msg, (10, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 0, 0), 2)
        cv2.putText(
            cv_img, msg, (10, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 0, 255), 1
        )

        cv2.imshow("img", cv_img)
        cv2.waitKey(1)

    def land(self, empty):
        msg = std_msgs.msg.Empty()
        self.land_pub.publish(msg)

    def command(self, args):
        if args[0] in ["l", "f", "r", "b"]:
            self.flip(*args)
        elif len(args) > 1:
            self.pub_velocity(*args)
        elif self.land_count > 5:
            self.land_count = 0
            self.land(*args)
    
    def pub_velocity(self, linear_x=0.0, linear_y=0.0, linear_z=0.0, angular_z=0.0):
        vel_msg = geometry_msgs.msg.Twist()

        vel_msg.angular.z = angular_z * self.angular_z_const
        vel_msg.linear.x = linear_x * self.linear_x_const
        vel_msg.linear.y = linear_y * self.linear_y_const
        vel_msg.linear.z = linear_z * self.linear_z_const

        self.cmd_vel_pub.publish(vel_msg)

    def flip(self, direction):
        msg = std_msgs.msg.String()
        msg.data = direction
        self.flip_pub.publish(msg)


def add_arguments(parser: argparse.ArgumentParser):
    parser.add_argument(
        "-m",
        "--use-model",
        default="False",
        type=str,
        nargs="?",
        dest="use_model",
        help="use neural network model for gesture recognition",
    )

    parser.add_argument(
        "-p",
        "--path",
        default="",
        type=str,
        nargs="?",
        dest="model_path",
        help="path to the neural network model",
    )


def parse_landmarks(landmarks):
    ret = []
    for landmark in landmarks.landmark:
        ret.append([landmark.x, landmark.y, landmark.z])

    return np.array(ret)


if __name__ == "__main__":
    rclpy.init()

    parser = argparse.ArgumentParser(description="Operate the drone using hand gestures.")
    add_arguments(parser)

    rosargv = remove_ros_args(sys.argv)[1:]
    args = parser.parse_args(rosargv)

    steering = GestureSteering(args.use_model.lower() == "true", args.model_path)

    try:
        rclpy.spin(steering)
    except KeyboardInterrupt as e:
        steering.get_logger().info("Got Ctrl+C, shutting down.")
        steering.land([])
        steering.destroy_node()
    finally:
        rclpy.shutdown()
